{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5666e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[133.33926307937517, -2.1222181382858807], [133.66987974088937, -2.169258037223057], [135.10146060637226, -2.3719827384727505], [135.15651022492466, -2.3795776032422102], [135.16921656996306, -2.380764974730238], [135.17710017210675, -2.3786771269659686], [135.1926766460784, -2.308126278823152], [135.24322299512, -2.0681480764632365], [135.3828317217139, -1.4039781820756891], [135.5132515675912, -0.7829590578866706], [135.51325041657466, -0.7734063250034875], [135.50834489604387, -0.7657268208508158], [135.49696210893498, -0.763272840571347], [135.44877859837575, -0.7556124598662443], [133.68167737410727, -0.5051547005707295], [133.67714173994176, -0.5196034692818913], [133.61264497309745, -0.8240968738741947], [133.47564446743897, -1.4709451740585702], [133.3383749266601, -2.1177255078764916], [133.33926307937517, -2.1222181382858807]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import ee\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "import io\n",
    "import urllib\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from httplib2 import Http\n",
    "from oauth2client import file, client, tools\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##############################################\n",
    "# USER SET PARAMETERS\n",
    "##############################################\n",
    "# set location to save preprocessed files\n",
    "save_folder_location = './preprocessed_data_temp/'\n",
    "# set the location of the mask resource on google earth engine - this is account dependent\n",
    "\n",
    "# period start and period end is for reference image (time t)\n",
    "# get 1 year historical satellite image (t - 52 weeks to t)\n",
    "# hotspot labels for 5th week into future is retrieved (t + 4weeks to t + 5 weeks)\n",
    "period_start = '2019-08-01'\n",
    "period_end = '2019-08-28'\n",
    "##############################################\n",
    "\n",
    "mask_feature_collection_path = 'USDOS/LSIB_SIMPLE/2017'\n",
    "\n",
    "\n",
    "# if folder does not exist, create it\n",
    "if not os.path.exists(save_folder_location):\n",
    "    os.makedirs(save_folder_location)\n",
    "\n",
    "##############################################\n",
    "# Initializing google earth engine and google drive API\n",
    "##############################################\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = 'https://www.googleapis.com/auth/drive'\n",
    "\n",
    "\n",
    "store = file.Storage('token.json')\n",
    "creds = store.get()\n",
    "if not creds or creds.invalid:\n",
    "    flow = client.flow_from_clientsecrets('credentials.json', SCOPES)\n",
    "    creds = tools.run_flow(flow, store)\n",
    "service = build('drive', 'v3', http=creds.authorize(Http()))\n",
    "\n",
    "\n",
    "\n",
    "# set the size of one side for each image for prediction (square), in number of pixels, 1 px = 30m * 30m for landsat 7\n",
    "# 66 * 30 ~ 2km, currently set to 8km * 8km area\n",
    "target_image_data_size = 66\n",
    "# set date format here for python datetime conversion (strptime and strftime)\n",
    "image_date_format = '%Y-%m-%d'\n",
    "\n",
    "# set time in the future for prediction - currently 30 days in advance\n",
    "prediction_time_distance = datetime.timedelta(days=30)\n",
    "# set to predict 1 week worth of fire\n",
    "prediction_interval_time_distance = datetime.timedelta(days=7)\n",
    "# set satellite data interval to 16 days - according to landsat 7 data frequency\n",
    "satellite_revisit_interval = datetime.timedelta(days=16)\n",
    "# set period in the past to find data for 1 piece of land - currently set to 365 day\n",
    "satellite_data_period = datetime.timedelta(days=365)\n",
    "\n",
    "# functions to handle downloading and preprocessing########################################################################\n",
    "def drive_export_and_preprocess(image, image_id, image_date):\n",
    "    '''\n",
    "    export image to google drive, download from google drive\n",
    "    and convert image to histogram and save\n",
    "    '''\n",
    "    drive_filename = image_date + image_id\n",
    "    taskname = 'export' + '-' + drive_filename\n",
    "    # ee.batch.Export.image(image, taskname, {})\n",
    "    # task = ee.batch.Export.image(image,\n",
    "    #     taskname, {\n",
    "    #         'driveFolder': 'gee_export_Data',\n",
    "    #         'driveFileNamePrefix': drive_filename,\n",
    "    #         'scale':30\n",
    "    #     }\n",
    "    # )\n",
    "    # run it at 1/4 res per side, 1/16 res in area\n",
    "    task = ee.batch.Export.image(image,\n",
    "        taskname, {\n",
    "            'driveFolder': 'gee_export_Data',\n",
    "            'driveFileNamePrefix': drive_filename,\n",
    "            'scale':120\n",
    "        }\n",
    "    )\n",
    "    # task = ee.batch.Export.image(image,\n",
    "    #     taskname, {\n",
    "    #         'driveFolder': 'gee_export_Data',\n",
    "    #         'driveFileNamePrefix': drive_filename,\n",
    "    #         'scale':60\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "\n",
    "    # this starts the task\n",
    "    task.start()\n",
    "    # wait until task is either done or failed\n",
    "    while(task.status()['state'] != 'COMPLETED'):\n",
    "        print(task.status()['state'] + ' ' + taskname)\n",
    "        if (task.status() == 'FAILED'):\n",
    "            print('FAILED ' + taskname)\n",
    "            break\n",
    "        else:\n",
    "            ### wait for 20 sec since task not completed\n",
    "            time.sleep(20)\n",
    "\n",
    "    # supposedly wait for file to appear in drive\n",
    "    time.sleep(20)\n",
    "\n",
    "    # assume task is done here, now to download from google drive\n",
    "    drive_search_query = \"mimeType='image/tiff' and (name contains '\" + drive_filename + \"')\"\n",
    "    # Call the Drive v3 API to search for files, can leave as .list() for everything\n",
    "    results = service.files().list(q=drive_search_query,\n",
    "        pageSize=10,\n",
    "        fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    drive_items = results.get('files', [])\n",
    "    drive_item = drive_items[0]\n",
    "\n",
    "    # to download, need to retrieve file id\n",
    "    file_id = drive_item['id']\n",
    "    # we need a file name too\n",
    "    file_name = drive_item['name']\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    fh = io.FileIO(file_name, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        # print('Download %d%%.' % int(status.progress() * 100))\n",
    "\n",
    "\n",
    "\n",
    "    # now we need to import into python and save the image as histograms\n",
    "    raster_tiff = gdal.Open(file_name, gdal.GA_ReadOnly)\n",
    "    num_layer = raster_tiff.RasterCount\n",
    "\n",
    "    # layer, height, width\n",
    "    combined_data_array = raster_tiff.ReadAsArray()\n",
    "    # change to height, width, layer, with label image on top\n",
    "    combined_data_array = combined_data_array.transpose(1, 2, 0)\n",
    "\n",
    "    # the first layer is always the label image\n",
    "\n",
    "    # label_layer = raster_tiff.GetRasterBand(1).ReadAsArray()\n",
    "    # result_layer_list = [label_layer]\n",
    "\n",
    "    # for i in range(2, num_layer + 1):\n",
    "    #     # read the individual layer and add it to list\n",
    "    #     current_layer = raster_tiff.GetRasterBand(i).ReadAsArray()\n",
    "    #     result_layer_list.append(current_layer)\n",
    "\n",
    "    # # stack elements of list into a single numpy array-(width, height, layer)\n",
    "    # stacked_data = np.stack(result_layer_list, axis=2)\n",
    "\n",
    "    # # free up some memory here\n",
    "    # del(result_layer_list)\n",
    "\n",
    "    ## now to iterate the stacked data into smaller pieces and convert to\n",
    "    ## histogram to save\n",
    "    convert_to_histogram(combined_data_array, file_name, target_image_data_size)\n",
    "    # convert_to_histogram(stacked_data, file_name, target_image_data_size)\n",
    "\n",
    "    # deleting the file on google drive to free up space\n",
    "    service.files().delete(fileId=file_id).execute()\n",
    "\n",
    "    # delete the main downloaded image file, since not needed\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "\n",
    "def convert_to_histogram(stacked_array, file_name, image_slice_length):\n",
    "    '''\n",
    "    Cuts the stacked array up into smaller pieces\n",
    "    calculates the histogram for each small piece\n",
    "    saves it using pickle\n",
    "    '''\n",
    "    length = stacked_array.shape[0]\n",
    "    width = stacked_array.shape[1]\n",
    "    depth = stacked_array.shape[2]\n",
    "    # using integer division because we want to floor it\n",
    "    num_slice_length = length // image_slice_length\n",
    "    num_slice_width = width // image_slice_length\n",
    "    # this counter helps locate where is the resultant histogram from later on, if needed\n",
    "    counter = 0\n",
    "    # histogram bin boundaries for numpy histogram conversion,\n",
    "    # we are using 32 bins of 8, ignoreing 0 values because undefined areas are 0\n",
    "    bin_boundaries = [1, 8, 16, 24, 32, 40, 48, 56, 64, 72,\n",
    "        80, 88, 96, 104, 112, 120, 128, 136, 144, 152,\n",
    "        160, 168, 176, 184, 192, 200, 208, 216, 224, 232,\n",
    "        240, 248, 255]\n",
    "\n",
    "    for i in range(num_slice_length):\n",
    "        for  j in range(num_slice_width):\n",
    "            ## calculating indices to slice at\n",
    "            length_lower = i * image_slice_length\n",
    "            length_upper = length_lower + image_slice_length\n",
    "\n",
    "            width_lower = j * image_slice_length\n",
    "            width_upper = width_lower + image_slice_length\n",
    "            # slice area of image_slice_length ** 2 along length and width, keep all depth(layers)\n",
    "            current_slice = stacked_array[length_lower:length_upper, width_lower:width_upper, :]\n",
    "\n",
    "            # if slice is all 0, ignore\n",
    "            if(np.max(current_slice) == 0):\n",
    "                continue\n",
    "            # separate the label and the data instance\n",
    "            current_slice_label = np.max(current_slice[:, :, 0])\n",
    "            current_slice = current_slice[:, :, 1:]\n",
    "\n",
    "            current_layer_list = []\n",
    "            for k in range(current_slice.shape[2]):\n",
    "                current_layer_array = current_slice[:, :, k]\n",
    "\n",
    "                current_layer_histogram, _ = np.histogram(current_layer_array, bins=bin_boundaries, range=(1, 255))\n",
    "                ## NOTE: only saving non 0 pixels, since 0 -> missing data,\n",
    "                ## real world almost no 0 pixels in images\n",
    "                current_layer_list.append(current_layer_histogram)\n",
    "\n",
    "            current_sliced_stacked_histogram = np.stack(current_layer_list, axis=1)\n",
    "\n",
    "            slice_filename = os.path.splitext(file_name)[0] + '_slice_' + str(counter) + '.pickle'\n",
    "            pickle.dump((current_sliced_stacked_histogram, current_slice_label),\n",
    "                open(save_folder_location + slice_filename, 'wb'))\n",
    "\n",
    "            counter = counter + 1\n",
    "    # done with processing slices\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "\n",
    "# indonesia_mask = ee.FeatureCollection(mask_feature_collection_path).filter(ee.Filter.eq(\"Name\", 'Indonesia'))\n",
    "# using new source of country boundary\n",
    "indonesia_mask = ee.FeatureCollection(mask_feature_collection_path).filter(ee.Filter.eq('country_na', 'Indonesia'))\n",
    "\n",
    "\n",
    "# for time period, get all landsat images within those bounds\n",
    "# period_start = '2019-07-01'\n",
    "# period_end = '2019-07-28'\n",
    "\n",
    "\n",
    "## select all satellite images of indonesia in that time bound\n",
    "## NOTE: might have to loop over bounds because max num of images seems to be 5000\n",
    "satellite_image_collection = ee.ImageCollection('LANDSAT/LE07/C01/T1_RT') \\\n",
    "    .filter(ee.Filter.date(period_start, period_end)) \\\n",
    "    .filterBounds(indonesia_mask.geometry()) \\\n",
    "    .sort('DATE_ACQUIRED', False)\n",
    "\n",
    "# print(satellite_image_collection.getInfo())\n",
    "\n",
    "def make_list(current_image, prev_list):\n",
    "    current_image = ee.Image(current_image)\n",
    "    prev_list = ee.List(prev_list)\n",
    "    new_list = prev_list.add(current_image)\n",
    "    return new_list\n",
    "\n",
    "image_list = satellite_image_collection.iterate(make_list, ee.List([]))\n",
    "\n",
    "image_list = ee.List(image_list)\n",
    "num_images = image_list.size().getInfo()\n",
    "\n",
    "# now we have a list of images that we want\n",
    "# for each image we want to find its past and roll it into a stacked image\n",
    "for i in range(num_images):\n",
    "    current_image = ee.Image(image_list.get(i))\n",
    "    coordinates = current_image.getInfo()['properties']['system:footprint']['coordinates']\n",
    "    print(coordinates)\n",
    "#     print(find_center(coordinates))\n",
    "#     print(current_image.getInfo()['properties']['system:footprint']['coordinates'])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "657f0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def find_center(coordinates: List[List[float]]) -> List[float]:\n",
    "    \"\"\"Finds the center of a list of coordinates.\n",
    "\n",
    "    Args:\n",
    "        coordinates: A list of lists containing longitude and latitude coordinates.\n",
    "\n",
    "    Returns:\n",
    "        A list containing the center longitude and latitude coordinates.\n",
    "    \"\"\"\n",
    "    # Calculate the average longitude and latitude\n",
    "    total_longitude = sum(coord[0] for coord in coordinates)\n",
    "    total_latitude = sum(coord[1] for coord in coordinates)\n",
    "    center_longitude = total_longitude / len(coordinates)\n",
    "    center_latitude = total_latitude / len(coordinates)\n",
    "\n",
    "    return [center_longitude, center_latitude]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b6a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
